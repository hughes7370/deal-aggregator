from backend.src.api.listings_api import get_all_listings
from backend.src.services.deal_analyzer import analyze_listings
from backend.src.services.listing_details_scraper import ListingDetailsScraper
from config.search_queries import get_queries_from_db
import json
from datetime import datetime, timedelta
from backend.src.database.supabase_db import SupabaseClient
import threading
import signal

# Global flag for scraper status
_scraper_running = False
_scraper_lock = threading.Lock()

def is_scraper_running():
    """Check if scraper is currently running"""
    with _scraper_lock:
        return _scraper_running

def set_scraper_running(status: bool):
    """Set scraper running status"""
    with _scraper_lock:
        global _scraper_running
        _scraper_running = status
        print(f"ğŸ”’ Scraper running status set to: {status}")

def run_with_timeout(func, timeout_seconds):
    """Run a function with a timeout"""
    def handler(signum, frame):
        print("âš ï¸ Timeout signal received - scraper execution exceeded time limit")
        raise TimeoutError("Function execution timed out")
    
    print(f"â±ï¸ Setting up timeout handler for {timeout_seconds} seconds")
    # Set up the timeout
    signal.signal(signal.SIGALRM, handler)
    signal.alarm(timeout_seconds)
    
    try:
        print("â–¶ï¸ Starting function execution with timeout")
        result = func()
        signal.alarm(0)  # Disable the alarm
        print("âœ… Function completed successfully within timeout")
        return result
    except TimeoutError as e:
        print("âŒ Function execution timed out")
        raise e
    finally:
        signal.alarm(0)  # Ensure the alarm is disabled
        print("ğŸ”„ Timeout handler cleanup completed")

def main():
    try:
        FINAL_LIMIT = 10  # Number of listings to analyze per platform
        print(f"Starting business listing search...")
        print(f"Process: Fetch listings â†’ Filter by criteria â†’ Analyze top {FINAL_LIMIT} matches")
        
        # Get static search URLs
        print("ğŸ“š Fetching search queries from database...")
        search_queries = get_queries_from_db()
        print(f"Found {len(search_queries)} search queries")
        
        # Get listing results (includes Empire Flippers)
        print("\nğŸ” Fetching listings from all platforms...")
        results = get_all_listings(limit=FINAL_LIMIT, queries=search_queries, debug=False)
        
        # Only proceed with analysis if we have results
        if results and any(results.values()):
            print("\nğŸ“Š Analyzing acquisition opportunities...")
            print("Results by platform:")
            for platform, listings in results.items():
                print(f"- {platform}: {len(listings)} listings")
            
            # Combine all listings into a single list
            all_listings = []
            for platform, listings in results.items():
                for listing in listings:
                    listing['source_platform'] = platform
                    all_listings.append(listing)
            
            if all_listings:
                # Enrich listings with detailed information
                print(f"\nğŸ” Enriching {len(all_listings)} listings with detailed information...")
                scraper = ListingDetailsScraper(debug=False)
                enriched_listings = scraper.enrich_listings(all_listings)
                
                # Generate analysis for listings
                if enriched_listings:
                    print(f"\nğŸ“ˆ Generating analysis for {len(enriched_listings)} enriched listings...")
                    analysis = analyze_listings(enriched_listings, debug=False)
                    
                    if analysis:
                        print("\nğŸ“‹ Acquisition Analysis:")
                        print(analysis['acquisition_analysis'])
                    else:
                        print("âŒ No analysis was generated.")
            else:
                print("âŒ No listings found to analyze.")
                
    except Exception as e:
        print(f"âŒ An error occurred: {str(e)}")
        import traceback
        print(f"ğŸ“œ Traceback:\n{traceback.format_exc()}")
        return None

def run_scrapers():
    """
    Run all scrapers and store results in the database
    """
    print("\nğŸ¤– Starting scraper execution...")
    print(f"â° Current time: {datetime.now().isoformat()}")
    
    # Check if scraper is already running
    if is_scraper_running():
        print("âš ï¸ Scraper is already running, skipping this run")
        return
    
    try:
        # Set scraper as running
        print("ğŸ”’ Setting scraper status to running")
        set_scraper_running(True)
        
        # Initialize database client
        print("ğŸ”Œ Initializing database connection...")
        db = SupabaseClient()
        
        def scraper_task():
            # Get listings from all sources
            print("\nğŸ”„ Starting scraper run...")
            print("ğŸ“Š Fetching listings from all platforms")
            try:
                all_listings = get_all_listings()
            except Exception as e:
                print(f"âŒ Error getting listings: {e}")
                print(f"ğŸ“œ Traceback:\n{traceback.format_exc()}")
                all_listings = {}  # Continue with empty listings rather than failing
            
            total_processed = 0
            total_errors = 0
            
            # Store listings in database
            for platform, listings in all_listings.items():
                try:
                    print(f"\nğŸ“¦ Processing {len(listings)} listings from {platform}")
                    platform_processed = 0
                    platform_errors = 0
                    
                    for listing in listings:
                        try:
                            print(f"ğŸ’¾ Storing listing: {listing.get('title', 'Unknown Title')[:100]}...")  # Truncate long titles
                            db.store_listing(listing)
                            platform_processed += 1
                            total_processed += 1
                        except Exception as e:
                            print(f"âŒ Error storing listing: {str(e)[:500]}")  # Truncate long error messages
                            platform_errors += 1
                            total_errors += 1
                            continue
                    
                    print(f"\nğŸ“Š Platform Summary - {platform}:")
                    print(f"âœ… Successfully stored: {platform_processed}")
                    print(f"âŒ Errors: {platform_errors}")
                except Exception as e:
                    print(f"âŒ Error processing platform {platform}: {e}")
                    print(f"ğŸ“œ Traceback:\n{traceback.format_exc()}")
                    continue
            
            print("\nğŸ“ˆ Overall Scraper Summary:")
            print(f"âœ… Total listings processed: {total_processed}")
            print(f"âŒ Total errors: {total_errors}")
            if total_processed > 0:
                print("âœ¨ Scraper run completed with some successes")
            else:
                print("âš ï¸ Scraper run completed but no listings were processed")
        
        # Run the scraper with a 45-minute timeout
        print("\nâ±ï¸ Starting scraper task with 45-minute timeout...")
        run_with_timeout(scraper_task, 45 * 60)  # 45 minutes in seconds
        
    except TimeoutError:
        print("âš ï¸ Scraper run timed out after 45 minutes")
    except Exception as e:
        print(f"âŒ Error in scraper run: {e}")
        import traceback
        print(f"ğŸ“œ Traceback:\n{traceback.format_exc()}")
        raise
    finally:
        # Always mark scraper as not running when done
        print("\nğŸ”“ Cleaning up: Setting scraper status to not running")
        set_scraper_running(False)
        print(f"ğŸ Scraper execution finished at: {datetime.now().isoformat()}")

if __name__ == "__main__":
    main()